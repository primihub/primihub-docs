---
sidebar_position: 5
description: åŸºäºChatGLM-6Bå®ç°æ¨ªå‘è”é‚¦å¤§æ¨¡å‹è®­ç»ƒ
keywords: [HFL, LLM, FedAvg]
---
# LLM

ç›®å‰ PrimiHub å·²æ”¯æŒåŸºäº ChatGLM-6B çš„è”é‚¦å¤§æ¨¡å‹ï¼Œæœ¬æ–‡æ¡£å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ PrimiHub å®ç°è”é‚¦å¤§æ¨¡å‹ã€‚

é…ç½®è¦æ±‚ï¼šæœ€ä½æ˜¾å­˜éœ€æ±‚ 7G

æ­¥éª¤åˆ†ä¸ºå‡ ä¸ªæµç¨‹ï¼š

1. éƒ¨ç½² PrimiHubï¼Œå‚è€ƒ[è¿™é‡Œ](https://docs.primihub.com/docs/advance-usage/start/build)
2. å®‰è£… PrimiHub SDKï¼Œå‚è€ƒ[è¿™é‡Œ](https://docs.primihub.com/docs/advance-usage/python-sdk/install)
3. éƒ¨ç½² ChatGLM-6B
4. å¯åŠ¨èŠ‚ç‚¹
5. å‘èµ·ä»»åŠ¡å¹¶æŸ¥çœ‹ç»“æœ

## éƒ¨ç½² ChatGLM-6B

ChatGLM è¯¦ç»†æ–‡æ¡£å¯å‚è€ƒ[è¿™é‡Œ](https://github.com/THUDM/ChatGLM-6B)ï¼Œåœ¨æ­¤å°†åšè¿›ä¸€æ­¥çš„è§£é‡Šï¼Œä»¥å¸®åŠ©ç”¨æˆ·å……åˆ†ç†è§£è¯¥æµç¨‹ã€‚

### æ ¹æ®è‡ªå·±çš„GPUå‹å·å®‰è£…é©±åŠ¨

å‚è€ƒ[å®˜ç½‘](https://www.nvidia.cn/Download/index.aspx?lang=cn)

### å®‰è£… CUDA ç‰ˆæœ¬ PyTorch

æ¨èä½¿ç”¨æ˜¾å¡è¿›è¡ŒåŠ é€Ÿï¼Œå¹¶å®‰è£… PyTorch ä»¥åŠç›¸åº”ç‰ˆæœ¬çš„ CUDAã€‚
é¦–å…ˆç¡®è®¤æœ¬åœ°å®‰è£…çš„ç‰ˆæœ¬ï¼Œå¦‚æœä¸º Trueï¼Œåˆ™å¯ä»¥è·³è¿‡è¯¥æ­¥éª¤

```python
import torch   
torch.__version__'2.0.0'   
torch.cuda.is_available()   
True  
```

å¦åˆ™ï¼Œéœ€å…ˆåœ¨ PyTorch [å®˜ç½‘](https://pytorch.org/) ç¡®è®¤ç›®å‰ GPU ç‰ˆæœ¬è¦æ±‚ï¼Œä¾‹å¦‚ä»¥ä¸‹é…ç½®ã€‚æ³¨æ„ï¼Œæ­¤æ—¶æœ€å¥½å…ˆç¡®è®¤ CUDA ç‰ˆæœ¬ï¼Œå†è¿›è¡Œ PyTorch å®‰è£…ã€‚
![img](/img/cuda1.png)

å¦‚æœæ²¡æœ‰ CUDAï¼Œå»ºè®®å®‰è£… CUDA 11.8ï¼Œå…·ä½“æ•™ç¨‹å¯è§[è¿™é‡Œ](https://developer.nvidia.com/cuda-11-8-0-download-archive)ï¼Œé€‰æ‹©ç³»ç»Ÿéœ€æ±‚ï¼Œåˆ™ä¼šç”Ÿæˆå®‰è£…å‘½ä»¤ã€‚
![img](/img/cuda2.png)
![img](/img/cuda3.png)

:::tip
å¦‚æœæ›¾ç»å®‰è£…è¿‡ CUDAï¼Œå®‰è£…å‰æœ€å¥½å°†æ­¤å‰å®‰è£…çš„ç‰ˆæœ¬åˆ é™¤å¹²å‡€ï¼Œæ¯”å¦‚ /usr/local ä¸‹çš„ CUDA å¯ä»¥ç›´æ¥åˆ é™¤æˆ–ä½¿ç”¨`apt-get purge nvidia*`
:::

å®‰è£…å®Œæˆåï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ç¡®è®¤ç‰ˆæœ¬å·ä»¥åŠæ˜¯å¦å®‰è£…æˆåŠŸã€‚å¦‚æœç»“æœæ˜¯11.8ï¼Œè¯´æ˜å®‰è£…æ­£ç¡®ã€‚

```bash
/usr/local/cuda/bin/nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Sep_21_10:33:58_PDT_2022
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0
```

### éƒ¨ç½² ChatGLM

```bash
git clone https://github.com/THUDM/ChatGLM-6B.git
cd ChatGLM-6B
pip3 install -r requirements.txt
```

ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š<https://huggingface.co/THUDM/chatglm-6b-int4/tree/main>
ä½œä¸ºåç»­æ¨¡å‹çš„è¾“å…¥ã€‚

é…ç½®å®Œæˆåï¼Œåœ¨ ChatGLM-6B ä¸‹ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æµ‹è¯•ã€‚æ³¨æ„ï¼š"THUDM/chatglm-6b" éœ€è¦æ›¿æ¢ä¸ºæœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚

ä¾‹å¦‚è¿™é‡Œçš„ç¤ºä¾‹æ˜¯"~/czl/chatglm-6b-int4"

```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
>>> response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
>>> print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:

1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
```

å¦‚æœä¸Šè¿°èƒ½å¤Ÿæ­£å¸¸è¿è¡Œï¼Œé‚£ä¹ˆï¼Œæ­å–œä½ ï¼Œåº“å®‰è£…ä¸æ¨¡å‹é¢„æµ‹æ²¡é—®é¢˜äº†ã€‚åç»­æˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚
å®‰è£…å¦‚ä¸‹ä¾èµ–ï¼š

```bash
pip3 install rouge_chinese nltk jieba datasets
```

å®‰è£…å®Œæˆåï¼Œåˆ™å¯è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒå¾ˆç®€å•ï¼Œåœ¨ ~/ChatGLM-6B/ptuning ä¸‹è¿è¡Œä¸‹è¿°ä»£ç å³å¯ã€‚

```bash
bash train.sh

cat train.sh
PRE_SEQ_LEN=128
LR=2e-2

CUDA_VISIBLE_DEVICES=0 python3 main.py \
    --do_train \
    --train_file AdvertiseGen/train.json \
    --validation_file AdvertiseGen/dev.json \
    --prompt_column content \
    --response_column summary \
    --overwrite_cache \
    --model_name_or_path  ~/czl/chatglm-6b-int4\
    --output_dir output/adgen-chatglm-6b-pt-$PRE_SEQ_LEN-$LR \
    --overwrite_output_dir \
    --max_source_length 64 \
    --max_target_length 64 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --predict_with_generate \
    --max_steps 4 \
    --logging_steps 1 \
    --save_steps 3 \
    --learning_rate $LR \
    --pre_seq_len $PRE_SEQ_LEN \
    --quantization_bit 4
```

å‚æ•°è¯´æ˜ï¼š

- train_file ä¸ºè®­ç»ƒæ•°æ®æ–‡ä»¶
- model_name_or_path ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„ä½ç½®
- output_dirä¸ºè½ç›˜çš„ä½ç½®
å…¶ä»–çš„å‚æ•°æ¯”è¾ƒå®¹æ˜“ï¼Œå¯ä»¥å‚è€ƒåŸé¡¹ç›®åœ°å€ã€‚ä½¿ç”¨å®˜æ–¹æ–‡æ¡£çš„æ•°æ®é›† AdvertiseGen å³å¯å®Œæˆè®­ç»ƒã€‚ç¡®è®¤è®­ç»ƒèƒ½å¤Ÿå®Œæˆåï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ PrimiHub å®ç°è”é‚¦è®­ç»ƒã€‚

## ä½¿ç”¨ PrimiHub å®ç°è”é‚¦å¤§æ¨¡å‹è®­ç»ƒ

é…ç½® Json æ–‡ä»¶ï¼Œä¿®æ”¹ example/FL/chatglm/ChatGlm.json å‚æ•°ï¼š

```json
{
    "party_info": {
        "Alice": {
            "ip": "172.21.1.58",
            "port": 50050,
            "use_tls": false
        },
        "Bob": {
            "ip": "172.21.1.63",
            "port": 50051,
            "use_tls": false
        },
        "Charlie": {
            "ip": "172.21.1.58",
            "port": 50052,
            "use_tls": false
        },
        "task_manager": "127.0.0.1:50050"
    },
    "component_params": {
        "roles": {
            "server": "Charlie",
            "client": [
                "Alice",
                "Bob"
            ]
        },
        "common_params": {
            "model": "Chat_glm",
            "process": "train",
            "aggration_iter": 2,
            "train_iter": 3
        },
        "role_params": {
            "Alice": {
                "path": "~/czl/ChatGLM-6B-Med/ptuning",
                "train_file": "Meddata/train.json",
                "validation_file": "Meddata/train.json",
                "prompt_column": "prompt",
                "response_column": "response",
                "history_column": "history",
                "model_name_or_path": "~/czl/chatglm-6b-int4",
                "output_dir": "output/Alice_result",
                "num_examples": 10
            },
            "Bob": {
                "path": "~/czl/ChatGLM-6B/ptuning",
                "train_file": "AdvertiseGen/train.json",
                "validation_file": "AdvertiseGen/dev.json",
                "prompt_column": "content",
                "response_column": "summary",
                "model_name_or_path": "~/czl/chatglm-6b-int4",
                "output_dir": "output/Bob_result",
                "num_examples": 10
            }
        }
    }
}
```

ç›¸å…³å‚æ•°é…ç½®è¯´æ˜å¦‚ä¸‹ï¼š

- party_infoï¼šè®¾ç½®æ¯ä¸ªèŠ‚ç‚¹çš„ IP å’Œåœ°å€
- component_paramsï¼šè®¾ç½®å‚æ•°ï¼Œå…¶ä¸­ï¼š
  - rolesï¼šè®¾ç½®æ¯ä¸€ä¸ªå‚ä¸æ–¹çš„è§’è‰²è¡Œä¸ºï¼Œä¾‹å¦‚ Alice å’Œ Bob æ˜¯ clientï¼ŒCharlie æ˜¯ server
  - common_paramsæ˜¯å…¬å…±å‚æ•°ï¼š
    - aggration_iterï¼šèšåˆå‡ æ¬¡
    - train_iter: æ¯ä¸€è½®èšåˆè®­ç»ƒå‡ è½®
  - role_paramsæ˜¯æ¯ä¸€æ–¹çš„å‚æ•°è®¾ç½®ï¼š
    - Path: æœ¬åœ° ChatGLM-6B é¡¹ç›®çš„è·¯å¾„
    - train_file, validation_file, prompt_column, response_column, history_column, model_name_or_path, output_dir: åŒ train.bash
    - num_examples: æ¯ä¸€æ–¹çš„æ•°æ®é‡

é…ç½®å®Œæˆåï¼Œé€šè¿‡ä¸‹è¿°ä»£ç å³å¯å‘èµ·ä»»åŠ¡ã€‚

```bash
submit example/FL/chatglm/ChatGlm.json
```

### è¿›è¡Œå¤§æ¨¡å‹çš„éƒ¨ç½²

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥åœ¨æ¯ä¸€æ–¹çš„ output è·¯å¾„ä¸‹ï¼Œçœ‹åˆ° checkpoint æ–‡ä»¶ï¼Œç±»ä¼¼ï¼š

```bash
primihub@primihub58:~/czl/ChatGLM-6B-Med/ptuning/output/Alice_result$ ls
all_results.json  checkpoint-3  trainer_state.json  train_results.json
primihub@primihub58:~/czl/ChatGLM-6B-Med/ptuning/output/Alice_result$
```

ä¹‹åï¼Œä½¿ç”¨``python primihub/FL/algorithm/chatglm/load_model.py``å³å¯ä½“éªŒæ¨¡å‹ã€‚
æ³¨æ„ä¿®æ”¹ä»¥ä¸‹ç›¸å…³ç›®å½•ï¼Œä»¥å’Œè‡ªå·±çš„ç›®å½•é€‚é…ï¼š

```python
import os
import platform
import signal

import torch

import os
import torch
from transformers import AutoConfig, AutoModel, AutoTokenizer

# è½½å…¥Tokenizer
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)

model_name_or_path = "~/czl/chatglm-6b-int4"
pre_seq_len = 128
quantization_bit = 4
CHECKPOINT_PATH = "~/czl/ChatGLM-6B-Med/ptuning/output/Alice_result/checkpoint-3"

```
